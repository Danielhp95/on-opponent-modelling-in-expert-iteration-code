experiment:
  algorithms:
    - expert_iteration_with_apprentice_opponent_modelling_500_GamesPerIter_20_Budget_50_Rollout_3_Epochs
      #- expert_iteration_debug
  environment: ['Connect4-v0', 'multiagent-sequential']
  experiment_id: 'BRExIt_Opponent_Modelling'
  desired_winrate: 0.8
  # ideally, a multiple of policy updates. Param :games_per iteration:
  training_episodes: 400
  benchmarking_episodes: 100
  num_envs: -1


# --------- Agent hyperparameters ------
cnn_arch: &cnn_architecture
    phi_arch: 'CNN'
    use_batch_normalization: True
    preprocessed_input_dimensions: [7, 6]
    channels: [3, 10, 20, 20, 20, 1]
    kernel_sizes: [3, 3, 3, 3, 3]
    paddings: [1, 1, 1, 1, 1]
    strides: [1, 1, 1, 1, 1]
    residual_connections: [[0,1], [1,2], [2,3], [3,4]]

agents:
  expert_iteration: &expert_iteration_no_opponent_modelling
    <<: *cnn_architecture
    use_agent_modelling: False
    use_agent_modelling_in_mcts: False
    use_apprentice_in_expert: True
    use_cuda: False
    games_per_iteration: 500
    # Dataset params
    initial_memory_size: 3000
    memory_size_increase_frequency: 50
    end_memory_size: 20000
    # MCTS config
    mcts_budget: 50
    mcts_rollout_budget: 100
    mcts_exploration_factor: 1.4142
    mcts_use_dirichlet: True
    mcts_dirichlet_alpha: 1.4142
    # Neural net config
    batch_size: 64
    num_epochs_per_iteration: 3
    learning_rate: 1.0e-3
    # NN: Feature extractor
    feature_extractor_arch: 'CNN'

  expert_iteration_debug:
    <<: *expert_iteration_no_opponent_modelling
    mcts_budget: 4
    mcts_rollout_budget: 100
    use_apprentice_in_expert: True
    use_dirichlet: False
    games_per_iteration: 10
    batch_size: 8

  expert_iteration_with_apprentice_opponent_modelling_500_GamesPerIter_20_Budget_50_Rollout_3_Epochs_64_BatchSize:
    <<: *expert_iteration_no_opponent_modelling
    mcts_budget: 20
    mcts_rollout_budget: 50
    games_per_iteration: 400
    num_epochs_per_iteration: 3
    batch_size: 64
    use_cuda: True
    use_agent_modelling: True
